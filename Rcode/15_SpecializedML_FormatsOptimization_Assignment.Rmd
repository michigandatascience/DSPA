---
title: "Data Science and Predictive Analytics (UMich HS650)"
subtitle: "<h2><u>Assessment: 15. Specialized Machine Learning Topics</u></h2>"
author: "<h3>SOCR/MIDAS (Ivo Dinov)</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
tags: [DSPA, SOCR, MIDAS, Big Dta, Predictive Analytics] 
output:
  html_document:
    theme: spacelab
    highlight: tango
    includes:
      before_body: SOCR_header.html
      after_body: SOCR_footer_tracker.html
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

# Working with website data

* Download the [Main SOCR Wiki Page](http://wiki.socr.umich.edu/index.php/Main_Page) and compare `RCurl` and `httr`
* Read and write XML for [SOCR Main Page](http://wiki.socr.umich.edu/index.php/Main_Page)
* Scrape and ingest the [SOCR Main Page](http://wiki.socr.umich.edu/index.php/Main_Page) information.

# Network data and visualization

* Download [03_les miserablese_GraphData.txt](https://umich.instructure.com/files/330389/download?download_frd=1)
* Visualize this undirected network graph
* Summarize the graph and explain the output
* Calculate the degree and the centrality of this graph
* Find out some important nodes (corresponding to novel characters)
* Will the results change if we assume the graph is directed?

# Data conversion and parallel computing 

* Download [CaseStudy12_ AdultsHeartAttack_Data.xlsx](https://umich.instructure.com/courses/38100/files/folder/Case_Studies?) or require online
* Load this data as data frame
* Use `Export()` or `write.xlsx()` to renew the xlsx file
* Use `rio` package to convert this ".xlsx"" file to ".csv"
* Generate generalizing tabular data structures
* Generate a `data.table`
* Create disk-based data frames and perform basic calculation
* Perform basic calculation on the last 5 columns as a big matrix
* Use	DIAGNOSIS, SEX, DRG, CHARGES, LOS and AGE to predict DIED with randomForest setting `ntree=20000`. Notice: sample without replacement to get as large as possible balanced dataset
* Run `train()` in `caret` and detect the execution time
* Detect cores and make proper number of clusters
* Rerun `train()` parallelized and compare the execute time
* Use `foreach` and `doMC` to design a parallelized random forest with `ntree=20000` and compare the execution time against linear sequential execution.