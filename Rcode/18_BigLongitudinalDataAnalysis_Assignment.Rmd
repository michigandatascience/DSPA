---
title: "Data Science and Predictive Analytics (UMich HS650)"
subtitle: "<h2><u>Assessment: 18. Big Longitudinal Data Analysis</u></h2>"
author: "<h3>SOCR/MIDAS (Ivo Dinov)</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
tags: [DSPA, SOCR, MIDAS, Big Dta, Predictive Analytics] 
output:
  html_document:
    theme: spacelab
    highlight: tango
    includes:
      before_body: SOCR_header.html
      after_body: SOCR_footer_tracker.html
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

# Imaging Data

Review the 3D/4D MRI imaging data discussion in [Chapter 3](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/03_DataVisualization.html). Extract the time courses of several timeseries at different 3D spatial locations, some near-by, some farther apart (distant voxels). Then, apply time-series analyses, report findings, determine if near-by or farther-apart voxels may be more correlated.

<details>
  <summary>Example of extracting timeseries from 4D fMRI data</summary>
  <p>
    <code>
      # See examples here: https://cran.r-project.org/web/packages/oro.nifti/vignettes/nifti.pdf
      
      library(oro.nifti)
      fMRIURL <- "http://socr.umich.edu/HTML5/BrainViewer/data/fMRI_FilteredData_4D.nii.gz"
      fMRIFile <- file.path(tempdir(), "fMRI_FilteredData_4D.nii.gz")
      download.file(fMRIURL, dest=fMRIFile, quiet=TRUE)
      (fMRIVolume <- readNIfTI(fMRIFile, reorient=FALSE))
      # dimensions: 64 x 64 x 21 x 180 ; 4mm x 4mm x 6mm x 3 sec 
      
      fMRIVolDims <- dim(fMRIVolume); fMRIVolDims
      time_dim <- fMRIVolDims[4]; time_dim
      
      hist(fMRIVolume)
      
      # To examine the time course of a specific 3D voxel (say the one at x=30, y=30, z=15):
      plot(fMRIVolume[30, 30, 10,], type='l', main="Time Series of 3D Voxel \n (x=30, y=30, z=15)", col="blue")
      
      x1 <- c(1:180)
      y1 <- loess(fMRIVolume[30, 30, 10,]~ x1, family = "gaussian")
      lines(x1, smooth(fMRIVolume[30, 30, 10,]), col = "red", lwd = 2)
      lines(ksmooth(x1, fMRIVolume[30, 30, 10,], kernel = "normal", bandwidth = 5), col = "green", lwd = 3)
    </code>
  </p>
</details>

# Tabuar data

Use [Google Web-Search Trends and Stock Market Data](http://wiki.socr.umich.edu/index.php/SOCR_Data_GoogleTrends_2005_2011) to:

* Plot time series for variable `Job`
* Apply `TTR` to smooth the original graph by month
* Determine the differencing parameter
* Decide the auto-regressive (AR) and moving average (MA) parameters
* Build an ARIMA model and Forecast the timecouse over the next 365 days (for 2012).

# Latent variables model

Use [Hand written English Letters data](https://umich.instructure.com/courses/38100/files/folder/Case_Studies/16_HandwrittenLetters) to:

* Explore the data and evaluate the correlations between covariates
* Justify to apply latent variable model
* Apply proper data convert and scale data
* Fit SEM on the data by adding proper latent variable
* Summarize and interpret the outputs
* Use the model you find above to fit GEE and GLMM model setting latent variable as response and compare AIC.