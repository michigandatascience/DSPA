---
title: "Data Science and Predictive Analytics (UMich HS650)"
subtitle: "<h2><u>Assignment 4: Linear Algebra &amp; Matrix Computing Assignment</u></h2>"
author: "<h3>SOCR/MIDAS (Ivo Dinov)</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
tags: [DSPA, SOCR, MIDAS, Big Dta, Predictive Analytics] 
output:
  html_document:
    theme: spacelab
    highlight: tango
    includes:
      before_body: SOCR_header.html
      after_body: SOCR_footer_tracker.html
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

# How is Matrix multiplication defined?

Validate that $(A_{k,n}\times B_{n,m})^T = (B^T_{m,n})\times (A^T_{n,k})$, both using math notation (first principles) and using R functions for some example matrices.

# Scalar vs. Matrix Multiplication
Demonstrate the differences between the scalar multiplication ($*$) and matrix multiplication ($\%*\%$).

# Matrix Equations

Write a simple matrix solver ($b = Ax$, i.e.,$x= A^{-1}b$) and validate its accuracy using the R command `solve(A,b)`. Solve this equation:
$$
\begin{align*}
2a - b + 2c &= 5\\
-a - 2b + c &= 3\\
a + b  - c &= 2
\end{align*}.
$$

# Least Square Estimation
Use the [SOCR Knee Pain dataset](http://wiki.socr.umich.edu/index.php/SOCR_Data_KneePainData_041409#Data_Table), extract the `RB = Right-Back` locations $(x,y)$, and fit in a linear model for vertical location ($y$) in terms of the horizontal location ($x$). Display the linear model on top of the scatter plot of the paired data.

# Matrix manipuation
Create a matrix $A$ with elements `seq(1, 15, length = 6)` and argument `nrow = 3`, add a row to this matrix; add two columns to A to obtain a new matrix $C_{4,4}$. Then generate a diagonal matrix $D$ with $dim = 4$ and elements `rnorm(4,0,1)`. Apply element wise addition, subtraction, multiplication and division to the matrices $C$ and $D$. Apply matrix multiplication to $D$ and $C$. Obtain the inverse of $C$ and compare the result to `ginv()`.

# Matrix Transpose
Validate the multiplicative transpose formula ($(A_{k,n}\cdot B_{n,m})^{T} = (B_{n,m})^{T}\cdot (A_{k,n})^{T}$), both using math notation, as well as using R calls, e.g., you can try `A = matrix(1:6,nrow=3)`, `B = matrix(2:7, nrow = 2)`.

# Sample Statistics
Use the [SOCR Data Iris Sepal Petal Classes](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_052511_IrisSepalPetalClasses) to compute sample mean and variance of each variables. Then calculate the sample covariance - use both math notation and R built-in functions.

# Eigenvalues and Eigenvectors
Generate a random matrix with `A = matrix(rnorm(9,0,1),nrow = 3)`, compute the eigenvalues and eigenvectors for $A$. Then try to solve this equation $det(A-\lambda I) = 0$, where $\lambda$ is a vector of length $3$. Compare $\lambda$ and the eigenvalues you computed above.

<details>
  <summary>Example of manual and automated calculations of eigen-spectra (eigen-values and eigen-vectors)</summary>
  <p>
    <code>
    # A <- matrix(rnorm(9,0,1),nrow = 3); A  # define a random design matrix, may generate complex solutions
    A <- matrix(c(0,1/4,1/4,3/4,0,1/4,1/4,3/4,1/2),3,3,byrow=T); A
    eigen_spectrum <- eigen(A); eigen_spectrum  # compute the eigen spectrum (eigen-values, $l$, and eigen-vectors, $v$), $ A \times v = l \times v$.
    B <- A-eigen(A)$values*diag(3); B   # compute B = (A - eigen_value \times I)
    det(A-eigen(A)\$values*diag(3)) # verrify that the det(A-eigen(A)\$values*diag(3)) is not trivial ($0$)
    A%*%eigen(A)$vector - eigen(A)$value*diag(3) # validate that $ A \times v = l \times v$.
    all.equal(A,  eigen(A)\$vector %*% diag(eigen(A)\$values) %*% solve(eigen(A)$vector))  # compare A = v*l*inv(v)
    all.equal(diag(3),  A%*%eigen(A)$vector - eigen(A)$values * eigen(A)$vector)
    # The last line compares I == AV - lambda\*v, mind the $*$ and $%*%$ scalar and matrix operators
    </code>
  </p>
</details>